{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7da7f162",
   "metadata": {},
   "source": [
    "# Flight Fare Prediction - Implementation Notebook\n",
    "\n",
    "This notebook follows the implementation workflow from setup to validation and basic tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfbe5b",
   "metadata": {},
   "source": [
    "## 1) Set Up Environment and Imports\n",
    "Import required Python libraries, define notebook-wide settings, and verify package versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "from src.config import RAW_DATA_PATH, PROCESSED_DATA_PATH, MODEL_COMPARISON_PATH\n",
    "from src.data_preprocessing import load_and_preprocess\n",
    "from src.eda import run_eda\n",
    "from src.train import load_split_data, train_baseline_models\n",
    "from src.tune import tune_models\n",
    "from src.interpret import plot_actual_vs_predicted, plot_residuals\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751c818c",
   "metadata": {},
   "source": [
    "## 2) Define Configuration and Constants\n",
    "Create a configuration block for paths, runtime parameters, and default constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"raw_data_path\": str(RAW_DATA_PATH),\n",
    "    \"processed_data_path\": str(PROCESSED_DATA_PATH),\n",
    "    \"comparison_path\": str(MODEL_COMPARISON_PATH),\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e5334d",
   "metadata": {},
   "source": [
    "## 3) Implement Core Functions\n",
    "Write first version of core helper functions and main processing logic with example calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893d9dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preview(path: str):\n",
    "    raw_df = pd.read_csv(path)\n",
    "    print(\"Shape:\", raw_df.shape)\n",
    "    display(raw_df.head())\n",
    "    display(raw_df.describe(include=\"all\"))\n",
    "    return raw_df\n",
    "\n",
    "\n",
    "def run_core_workflow(path: str):\n",
    "    bundle = load_and_preprocess(path=path, save_processed=True)\n",
    "    eda_outputs = run_eda(bundle.df)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, cat_cols, num_cols = load_split_data(path)\n",
    "    baseline_df, baseline_models, baseline_preds = train_baseline_models(\n",
    "        X_train, X_test, y_train, y_test, cat_cols, num_cols\n",
    "    )\n",
    "    tuned_df, tuned_models = tune_models(\n",
    "        X_train, X_test, y_train, y_test, cat_cols, num_cols\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"bundle\": bundle,\n",
    "        \"eda_outputs\": eda_outputs,\n",
    "        \"baseline_df\": baseline_df,\n",
    "        \"tuned_df\": tuned_df,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "        \"baseline_models\": baseline_models,\n",
    "        \"tuned_models\": tuned_models,\n",
    "        \"baseline_preds\": baseline_preds,\n",
    "    }\n",
    "\n",
    "# Example call (uncomment after dataset is available)\n",
    "# preview_df = load_preview(CONFIG[\"raw_data_path\"])\n",
    "# workflow_result = run_core_workflow(CONFIG[\"raw_data_path\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f9a124",
   "metadata": {},
   "source": [
    "## 4) Add Input Validation and Error Handling\n",
    "Implement checks for invalid inputs, missing files, and boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20872d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def validate_input_path(path: str):\n",
    "    path_obj = Path(path)\n",
    "    if not path_obj.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found at: {path}\")\n",
    "    if path_obj.suffix.lower() != \".csv\":\n",
    "        raise ValueError(\"Input dataset must be a .csv file\")\n",
    "    return path_obj\n",
    "\n",
    "\n",
    "def validate_min_rows(df: pd.DataFrame, min_rows: int = 20):\n",
    "    if df.shape[0] < min_rows:\n",
    "        raise ValueError(f\"Dataset has too few rows ({df.shape[0]}). Minimum required: {min_rows}\")\n",
    "\n",
    "\n",
    "# Example validation usage\n",
    "# validated_path = validate_input_path(CONFIG[\"raw_data_path\"])\n",
    "# raw_df = pd.read_csv(validated_path)\n",
    "# validate_min_rows(raw_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61a74a",
   "metadata": {},
   "source": [
    "## 5) Run the Main Workflow\n",
    "Assemble functions into an executable workflow and run on sample input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e9dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute only after placing dataset in data/raw/\n",
    "try:\n",
    "    validated = validate_input_path(CONFIG[\"raw_data_path\"])\n",
    "    raw_df = pd.read_csv(validated)\n",
    "    validate_min_rows(raw_df)\n",
    "\n",
    "    workflow_result = run_core_workflow(str(validated))\n",
    "    display(workflow_result[\"baseline_df\"])\n",
    "    display(workflow_result[\"tuned_df\"])\n",
    "\n",
    "    best_baseline = workflow_result[\"baseline_df\"].sort_values(\"rmse\").iloc[0]\n",
    "    print(\"Best baseline model:\", best_baseline[\"model\"])\n",
    "except Exception as exc:\n",
    "    print(\"Workflow run skipped/failed:\", exc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3f72",
   "metadata": {},
   "source": [
    "## 6) Add Basic Unit Tests\n",
    "Create lightweight tests for success and failure cases and run them in-notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed445df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assert_raises(func, expected_exception, *args, **kwargs):\n",
    "    try:\n",
    "        func(*args, **kwargs)\n",
    "    except expected_exception:\n",
    "        return True\n",
    "    except Exception as wrong_exc:\n",
    "        raise AssertionError(f\"Expected {expected_exception}, got {type(wrong_exc)}\") from wrong_exc\n",
    "    raise AssertionError(f\"Expected {expected_exception} but no exception was raised\")\n",
    "\n",
    "\n",
    "def run_basic_tests():\n",
    "    # Failure case: bad extension\n",
    "    assert _assert_raises(validate_input_path, ValueError, \"not_a_csv.txt\")\n",
    "\n",
    "    # Success case: dataframe minimum rows check\n",
    "    test_df = pd.DataFrame({\"x\": list(range(25))})\n",
    "    validate_min_rows(test_df, min_rows=20)\n",
    "\n",
    "    # Failure case: not enough rows\n",
    "    small_df = pd.DataFrame({\"x\": [1, 2]})\n",
    "    assert _assert_raises(validate_min_rows, ValueError, small_df, 20)\n",
    "\n",
    "    print(\"Basic notebook tests passed\")\n",
    "\n",
    "\n",
    "run_basic_tests()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
